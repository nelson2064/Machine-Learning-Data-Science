{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b44ea0",
   "metadata": {},
   "source": [
    "## 4. Evaluating a machine learning model\n",
    "\n",
    "Three ways to evaluate Scikit-Learn models/estimators: \n",
    "\n",
    "1. Estimator's built-in `score()` method\n",
    "2. The `scoring` parameter\n",
    "3. Problem-specific metric functions\n",
    "    \n",
    "You can read more about these here: https://scikit-learn.org/stable/modules/model_evaluation.html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967ac1f",
   "metadata": {},
   "source": [
    "### 4.1 Evaluating a model with the `score` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data (be sure to click \"raw\") - https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/data/heart-disease.csv \n",
    "heart_disease = pd.read_csv(\"https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/data/heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Fit the model to the data (training the machine learning model)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The highest value for the .score() method is 1.0  , the lowest is 0.0\n",
    "clf.score(X_train, y_train)\n",
    "#the default score method for a classfication algorithm is accuracy \n",
    "#of course we have to convert into percentage if we want accuracy into percentage so 1.0 is 100% accuracy so if 0.8 then 80% accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd439c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why is our model getting 1.0 on the training data well it's had exposure to all fo the the training features and all of the training labels and so if the model is powerful enought , it iwll achieve a perfect socre on the traing data because its able to split data in our case, it's binary zero and one it's able to predcit from all of the X train value to  predict perfectly all o fthe y train values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c560dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48080ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now it might be a differnet scenario for the test data it hasn't seen the test sample  learn patterns in data that we have existing to make predcitions, quality predictions on data that data we have't seen before\n",
    "# the core is less on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the training score is usually sometimes quite a bit higher than the test score, but they should be relatively close but the training will generally be higher than the testing score and if you ever get a perfect test score, like 100% accuracy or something like that , always be skeptical go back and check your data but if we made our model a little bit worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c581acc",
   "metadata": {},
   "source": [
    "Let's use the 'score()' on our regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538636a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b15b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "housing_df[\"target\"] = housing[\"target\"]\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create model instance\n",
    "model = RandomForestRegressor(n_estimators=1000)\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f950980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test , y_test) # if we want to have a look at what the metric here that's being used for the score method how might we do that shift + tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean() #if we go the mean of that mean, if every single sample in our predictions redicted that we'd get an r-squared value of zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0dfd6",
   "metadata": {},
   "source": [
    "#we've seen how to quickly get a sniff of how our machine learning model is doing and evaluate it using the score method and that'll return a default evalutation metric depending on the problem we're working in regression it's returns ot the coeffiecient of determination and in classfication it returns the mean accuracy\n",
    "\n",
    "## However, when you get furhter into a problem, it's likely you'll want to start using osme more powerful metrics to evaluate your mdoel's performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb2b95",
   "metadata": {},
   "source": [
    "## 4.2 Evaluating a model using the scoring parmaeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross_val_score from the model_selection module\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Import the RandomForestClassifier model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the data into X (features/data) and y (target/labels)\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the model (on the training set)\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Call the fit method on the model and pass it training data\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12689103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using score()\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e37faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cross_val_score()\n",
    "cross_val_score(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01562f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross val return the array and score returns a single number "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a9e8b",
   "metadata": {},
   "source": [
    "## so cross val score returns the array because what cross validation does ? \n",
    "## it does 5 different split  > cross validation trained on 5 differnet versions of training data and evaluated on 5 different versions of the test data\n",
    "##so whats the purpose of this ? \n",
    ">well as you could imagine, if we're only training one model, it could be a lucky split , like say this 80% of rows say that hasd a whole bunch of information and the model was able to learn really well on these 80 rows, on these 80 patient records and then it got a really good score on this test set is that a tru reflection of how our model woruld understand the data or figure out the patterns in the data well not really because it just luck somehow easy patient record get and get a good score we are thinking our model is good as it is not\n",
    "so that where corss validation comes into play it aims to provide a soultuin to not training on all the data and avoding getting those lucky scores on just a single split of data so it will create 5 differnt split so no matter what our model is going to e triaaning on all of the data and evaluate on all of the data\n",
    "so it gives a number of 5 array\n",
    "it is going to differetn 5 fold split you can do 100 fold also but the recommened is 5 \n",
    "<img src = \"./Screenshot (87).png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cross_val_score()\n",
    "cross_val_score(clf, X, y , cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca37e9",
   "metadata": {},
   "source": [
    "Since we set `cv=5` (5-fold cross-validation), we get back 5 different scores instead of 1.\n",
    "\n",
    "Taking the mean of this array gives us a more in-depth idea of how our model is performing by converting the 5 scores into one.\n",
    "\n",
    "Notice, the average `cross_val_score()` is slightly lower than single value returned by `score()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cbfd9",
   "metadata": {},
   "source": [
    " and so what we do here is to figure out a more ideal performance metric or evaluation metric for our model is that we can take the average of this 5 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Single training and test split score\n",
    "clf_single_score = clf.score(X_test, y_test)\n",
    "\n",
    "# Take mean of 5-fold cross-validation\n",
    "clf_cross_val_score = np.mean(cross_val_score(clf, X, y, cv=5))\n",
    "\n",
    "#Compare the two\n",
    "clf_single_score, clf_cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36044c8f",
   "metadata": {},
   "source": [
    "## >>>>>>>> in this case, if you are asked to report the accuracy of your model even though it is lower you'd prefer the cross validation metric over the non cross validation metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e29ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X, y, cv=5, scoring=None) # default scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66def22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring parmaeter set to None by default\n",
    "cross_val_score(clf, X, y, cv=5, scoring=None) # default scoring         #so that means when we have scoring set to none its' going to use the default evaluation metric for cross-validation on our classifier \n",
    "#i f none a single value if none the estimate is default sccorer if available is used \n",
    "# now this is why we know that this is accuracy because if the scoring parmaeter of cross val score  is none, it uses the default scoring parmaeter of our estimate in our case  \n",
    "#so that means when we have scoring set to none it's going to use the default evaluation metric for corss validation on our classifier \n",
    "#Default socring parameter of classifier = mean accuracy\n",
    "\n",
    "# clf.score()\n",
    "\n",
    " # so it's going to return the same values or it might be slightly differnet, right because we haven't se tup a seed in this cell  so these value are going to be differnet to the cross cell score we see up there if we'd run it in here, we woruld have seen simlar values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870be6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default socring parameter of classifier = mean accuracy\n",
    "clf.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e787e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we gona have a look on a next few vidoes some other classification model evaluation metrices we can use  with cross val score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61950c7c",
   "metadata": {},
   "source": [
    "so why we use cross validation ?\n",
    "well as we so picture corss validation aim to solve not training  on all the data we are creating 5 models having model train on all of the data and avoiding getting lucky score so training on a single split and we so that in action tat clf socre is slightly higher then the cross value average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d842d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "916ad47d",
   "metadata": {},
   "source": [
    "### 4.2.1 Classification model evaluation metrics\n",
    "\n",
    "Four of the main evaluation metrics/methods you'll come across for classification models are:\n",
    "\n",
    "1. Accuracy\n",
    "2. Area under ROC curve\n",
    "3. Confusion matrix\n",
    "4. Classification report\n",
    "\n",
    "Let's have a look at each of these. We'll bring down the classification code from above to go through some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10a166",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e06379",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross_val_score from the model_selection module\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "coss_val_score = cross_val_score(clf , X , y , cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(coss_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ae4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy as percentage\n",
    "print(f\"Heart Disease Classifier Cross-Validated  Accuracy: {np.mean(coss_val_score) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy is saying given a random sample that the model hasn't seen before how likely to predict the right label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e7491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we just cover accuracy so you might be thinking why just not leve that as we start to go through other matrix here you will start to understand why  might be important to get few differnet evaluation metrix rather then just accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9607c",
   "metadata": {},
   "source": [
    "#### Area Under Receiver Operating Characteristic (ROC) Curve (AUC) Curve\n",
    ">now what does roc curve measure a rock curve is a comparision of a models true positive rate aka TPR versus a model's \n",
    "false postitive rate\n",
    "\n",
    "\n",
    "If this one sounds like a mouthful, its because reading the full name is.\n",
    "\n",
    "It's usually referred to as AUC for Area Under Curve and the curve they're talking about is the Receiver Operating Characteristic or ROC for short.\n",
    "\n",
    "So if hear someone talking about AUC or ROC, they're probably talking about what follows.\n",
    "\n",
    "\n",
    "\n",
    "ROC curves are a comparison of true postive rate (tpr) versus false positive rate (fpr).\n",
    "\n",
    "For clarity:\n",
    "* True positive = model predicts 1 when truth is 1\n",
    "* False positive = model predicts 1 when truth is 0\n",
    "* True negative = model predicts 0 when truth is 0\n",
    "* False negative = model predicts 0 when truth is 1\n",
    "\n",
    "Now we know this, let's see one. Scikit-Learn lets you calculate the information required for a ROC curve using the [`roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2143dea",
   "metadata": {},
   "source": [
    "## Area under the reciever operating characteristic curve (AUG/ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X_test ... etc\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "#FIt the classifier\n",
    "clf.fit(X_train , y_train)\n",
    "\n",
    "# Make predictions with probabilities\n",
    "y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "y_probs[:10], len(y_probs)\n",
    "\n",
    "\n",
    "# ROC curves are a comparison of true postive rate (tpr) versus false positive rate (fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the probabilites of the positive class only\n",
    "\n",
    "\n",
    "y_probs_positive = y_probs[:, 1]\n",
    "y_probs_positive[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fpr, tpr and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)\n",
    "\n",
    "# Check the false positive rate\n",
    "fpr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6da1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so looking at this doesn't make any sense but plotting it and seeing the roc curve acual roc curve gona make sense\n",
    "#just see an example how we might create roc curve plotting function      \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    \"\"\"\n",
    "    Plots a ROC curve given the false positve rate (fpr) and \n",
    "    true postive rate (tpr) of a classifier.\n",
    "    \"\"\"\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    # Plot line with no predictive power (baseline)\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Guessing')\n",
    "    # Customize the plot\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_roc_curve(fpr, tpr) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0ea1f",
   "metadata": {},
   "source": [
    "#if the false postive rate is 0.6 then the true postive rate will became 1.0\n",
    "the maximum score we can get here is 1.0 up here this model here going from  corner to corner is guessing can you guess where most the ideal rock curve might end up if this is guessing and our model is doing far better then guessing by getting about 80% 85% something like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1af3928",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#auc score are under curve what is auc score ?\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#auc stand for area under curve it is the area under the curve  if you remove the guessing curve for a second curve the areea occupie by the curve is the auc are under curve\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#it can goes up to 1.0\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[1;32m----> 6\u001b[0m roc_auc_score(\u001b[43my_test\u001b[49m, y_probs_positive)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#auc score are under curve what is auc score ?\n",
    "#auc stand for area under curve it is the area under the curve  if you remove the guessing curve for a second curve the areea occupie by the curve is the auc are under curve\n",
    "#it can goes up to 1.0\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, y_probs_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d6d5608",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roc_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#lets check the perfect roc curve  i have previously said it can go to 1.0\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# the are under the curve will be 1.0 auc will be 1.0 we just disccus that \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# but so in reality, a perfect rock curve is very unlikely that means you've got a perfect model it's got no flase positives everything's a true postive\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot perfect ROC curve\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m(y_test, y_test)\n\u001b[0;32m      6\u001b[0m plot_roc_curve(fpr, tpr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'roc_curve' is not defined"
     ]
    }
   ],
   "source": [
    "#lets check the perfect roc curve  i have previously said it can go to 1.0\n",
    "# the are under the curve will be 1.0 auc will be 1.0 we just disccus that \n",
    "# but so in reality, a perfect rock curve is very unlikely that means you've got a perfect model it's got no flase positives everything's a true postive\n",
    "# Plot perfect ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test)\n",
    "plot_roc_curve(fpr, tpr)\n",
    "#themain details ehre that a rock curve is predicting is  a ture positive rate versus a false postive rate\n",
    "#the main metric you can use to boil it down rathen than just being a curve , you can use the auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e80e83d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perfect ROC AUC score\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m roc_auc_score(\u001b[43my_test\u001b[49m, y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Perfect ROC AUC score\n",
    "roc_auc_score(y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff1e3f",
   "metadata": {},
   "source": [
    "In reality, a perfect ROC curve is unlikely.\n",
    "\n",
    "#### Confusion matrix\n",
    "The next way to evaluate a classification model is by using a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). \n",
    "\n",
    "A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict. In essence, giving you an idea of where the model is getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c20dac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m----> 3\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      5\u001b[0m confusion_matrix(y_test, y_preds)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d8d9a",
   "metadata": {},
   "source": [
    "Again, this is probably easier visualized.\n",
    "\n",
    "One way to do it is with `pd.crosstab()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "567252f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#visualize confusin matrix with pd.crosstab()\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pd\u001b[38;5;241m.\u001b[39mcrosstab(\u001b[43my_test\u001b[49m, \n\u001b[0;32m      3\u001b[0m             y_preds, \n\u001b[0;32m      4\u001b[0m             rownames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual Label\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m      5\u001b[0m             colnames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Label\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "#visualize confusin matrix with pd.crosstab()\n",
    "pd.crosstab(y_test, \n",
    "            y_preds, \n",
    "            rownames=[\"Actual Label\"], \n",
    "            colnames=[\"Predicted Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9c14be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our model has predicted 0 and 1 are the predicted lable #the row are the actual lables and the columns are the predictive labels\n",
    "#in this case actual lable is o and pri0dictive label is 0 we have 24 examples and where the preditive labels is 1 and the acutal labels is 1 we have 24 examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d84aa527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24 + 5 + 3 + 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88aa37d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43my_preds\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_preds' is not defined"
     ]
    }
   ],
   "source": [
    "len(y_preds) #lets check how many prediction we have done and why 61 because we have 61 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fa23209",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# why 61 because there is 61 examples on test \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43my_test\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# why 61 because there is 61 examples on test \n",
    "len(y_test) #we have made 61 prediction because we have 61 example on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a193e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other thing is that in actual label 2 nd column we can see 0 it is because the actual label of 5 data set is 0 but it predicts 1 therefore\n",
    "#a$#the acutal label is 1 but it predict 0 in 3 data set\n",
    "\n",
    "## hence confusin metrix came here these example there our model is being confusion  thes example our model is getting confused to prediciting 0 with actual  label is 1 or prdicting 1 where the actual is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5218f",
   "metadata": {},
   "source": [
    "<img src = \"./Screenshot (88).png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465cfb70",
   "metadata": {},
   "source": [
    "Make our confusion matrix more visual with  seaborn's heatmap\n",
    "sea born is a visualization library that is built on the top of matplotlib and it is prettry relatively easy to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a252801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c587f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR conda.notices.fetch:get_channel_notice_response(63): Request error <HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/r/notices.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018E42D9F550>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))> for channel: defaults url: https://repo.anaconda.com/pkgs/r/notices.json\n",
      "ERROR conda.notices.fetch:get_channel_notice_response(63): Request error <HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/main/notices.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018E42D9F850>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))> for channel: defaults url: https://repo.anaconda.com/pkgs/main/notices.json\n",
      "ERROR conda.notices.fetch:get_channel_notice_response(63): Request error <HTTPSConnectionPool(host='repo.anaconda.com', port=443): Max retries exceeded with url: /pkgs/msys2/notices.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000018E42D9F6D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))> for channel: defaults url: https://repo.anaconda.com/pkgs/msys2/notices.json\n",
      "\n",
      "EnvironmentLocationNotFound: Not a conda environment: C:\\Users\\Learn To Earn\\Documents\\GitHub\\Machine-Learning-Data-Science\\fScikitlearn Creating Machine Learning Models\\(sys.prefix)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "#how to install a conda package into the current environment from a jupyter notebook \n",
    "import sys #let us acces our computer gives us to acces system\n",
    "!conda install --yes --prefix (sys.prefix) seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6314d909",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m sns\u001b[38;5;241m.\u001b[39mset(font_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#create a confusion matrix\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m conf_mat \u001b[38;5;241m=\u001b[39m confusion_matrix(\u001b[43my_test\u001b[49m , y_preds)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Flot it using Seaborn\u001b[39;00m\n\u001b[0;32m     12\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(conf_mat)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "#Make our confusion matrix more visual with Seaborn's heatmap()\n",
    "\n",
    "import seaborn as  sns\n",
    "\n",
    "#Set the font scale\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "#create a confusion matrix\n",
    "conf_mat = confusion_matrix(y_test , y_preds)\n",
    "\n",
    "#Flot it using Seaborn\n",
    "sns.heatmap(conf_mat);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c8b7c",
   "metadata": {},
   "source": [
    "In reality, a perfect ROC curve is unlikely.\n",
    "\n",
    "#### Confusion matrix\n",
    "The next way to evaluate a classification model is by using a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). \n",
    "\n",
    "A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict. In essence, giving you an idea of where the model is getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d293e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, \n",
    "            y_preds, \n",
    "            rownames=[\"Actual Label\"], \n",
    "            colnames=[\"Predicted Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f5468",
   "metadata": {},
   "source": [
    "####new version this is a new version \n",
    "\n",
    "\n",
    "\n",
    "## creating a confusion matrix using scikit-learn\n",
    "to use the new methods of creating a confusion matrix with scikit-learn you will need sklearnversion 1.0+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9040c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a05b46",
   "metadata": {},
   "source": [
    "Creating a confusion matrix using Scikit-Learn¶\n",
    "Scikit-Learn has multiple different implementations of plotting confusion matrices:\n",
    "\n",
    "sklearn.metrics.ConfusionMatrixDisplay.from_estimator(estimator, X, y) - this takes a fitted estimator (like our clf model), features (X) and labels (y), it then uses the trained estimator to make predictions on X and compares the predictions to y by displaying a confusion matrix.\n",
    "sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_true, y_pred) - this takes truth labels and predicted labels and compares them by displaying a confusion matrix.\n",
    "Note: Both of these methods/classes require Scikit-Learn 1.0+. To check your version of Scikit-Learn run:\n",
    "\n",
    "import sklearn\n",
    "sklearn.__version__\n",
    "If you don't have 1.0+, you can upgrade at: https://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need trained or fitted estimator\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=clf, X=X, y=y);\n",
    "\n",
    "         #   above version takes y true and y predic   just like corss tab before you have predcition ready to go but in this you don't have to ready for prediction in this confusion matrxi you don't need  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix from predictions\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test, \n",
    "                                        y_pred=y_preds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73430196",
   "metadata": {},
   "source": [
    "## Classification Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b4eb69",
   "metadata": {},
   "source": [
    "#### Classification report\n",
    "\n",
    "The final major metric you should consider when evaluating a classification model is a classification report.\n",
    "\n",
    "A classification report is more so a collection of metrics rather than a single one.\n",
    "\n",
    "You can create a classification report using Scikit-Learn's [`classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function.\n",
    "\n",
    "Let's see one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820be351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#again comparing true lable of the data vs the prediction that our model has made\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff790e",
   "metadata": {},
   "source": [
    "<img src = \"./Screenshot (90).png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10342821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#when i should use each of these \n",
    " #why not only use accuracy well lets have an example let's see when other metrics come into play and maybe you shouldn't just use accuracy because this is a trap that i got caught in right when i first started building classification model my model is getting 99% accuracy it must be a great model  lets see a senarion \n",
    "\n",
    "# Where precision and recall become valuable\n",
    "#in fact all the metrics in our classification report becomes valuable here\n",
    "disease_true = np.zeros(10000)\n",
    "disease_true[0] = 1 # only one case #only one postive case  and  #1 postitive case in 10000\n",
    "\n",
    "disease_preds = np.zeros(10000) # every prediction is 0 #it means the model pridcits every case as 0 #and we build a model and our model predcts that every single case is zero so it misses the one pridcition\n",
    "\n",
    "pd.DataFrame(classification_report(disease_true, \n",
    "                                   disease_preds, \n",
    "                                   output_dict=True,\n",
    "                                   zero_division=0))\n",
    "\n",
    "\n",
    "#so this is a prime example , righ t, of where you want to use another metric othe than accuracy is when you have a very large class imbalance so in our case we have a  massive class imbalacne because in our orighinal data set, does these equals true ony one example where the label is 1 and other is 0 we build a model that pridict zero for every case because it just it's only one smaple so it's really hard to learn that there's a pattern there for this one particualr case so what happesn if if we were to measure just accuracy on our model that is predicted zeor for everything it comes out with an accuracy of 0.99 or in other words, 99% and so ask yourself, althought the model achiever 99.99% accuracy, is the model still userful right that why why we look to other matrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80106a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4584f1fd",
   "metadata": {},
   "source": [
    "### 4.2.2 Regression model evaluation metrics\n",
    "\n",
    "Similar to classification, there are [several metrics you can use to evaluate your regression models](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics).\n",
    "\n",
    "We'll check out the following.\n",
    "\n",
    "1. **R^2 (pronounced r-squared) or coefficient of determination** - Compares your models predictions to the mean of the targets. Values can range from negative infinity (a very poor model) to 1. For example, if all your model does is predict the mean of the targets, its R^2 value would be 0. And if your model perfectly predicts a range of numbers it's R^2 value would be 1. \n",
    "2. **Mean absolute error (MAE)** - The average of the absolute differences between predictions and actual values. It gives you an idea of how wrong your predictions were.\n",
    "3. **Mean squared error (MSE)** - The average squared differences between predictions and actual values. Squaring the errors removes negative errors. It also amplifies outliers (samples which have larger errors).\n",
    "\n",
    "Let's see them in action. First, we'll bring down our regression model code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RandomForestRegressor model class from the ensemble module\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the data\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Institate and fit the model (on the training set)\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b205e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "**R^2 Score (coefficient of determination)**\n",
    "\n",
    "Once you've got a trained regression model, the default evaluation metric in the `score()` function is R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the models R^2 score\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a55271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc2b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8e237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e5956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bffb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
